import streamlit as st
import hashlib
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.documents import Document
from models import get_models
from retriever import Retriever
from summarizer import Summarizer
from external_search import fetch_crossref, fetch_arxiv, cache_save, cache_load, is_recommend_command, parse_recommend_command
from agents import build_paper_compare_agent
from cleaner import clean_documents, extract_search_query_from_docs
import os

MAX_HISTORY_ROUNDS = 5


def single_doc_qa_tab(llm, embeddings):
    st.subheader("ğŸ“„ å•ç¯‡è®ºæ–‡æ™ºèƒ½é—®ç­”")

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "summary" not in st.session_state:
        st.session_state.summary = ""

    summarizer = Summarizer(llm)
    retriever = Retriever(llm, embeddings)

    uploaded_file = st.file_uploader("ä¸Šä¼ ä½ çš„è®ºæ–‡PDFæ–‡ä»¶", type='pdf', key="single_doc_uploader")

    if uploaded_file:
        file_content = uploaded_file.read()
        temp_file_path = "temp_single.pdf"
        with open(temp_file_path, "wb") as temp_file:
            temp_file.write(file_content)
        loader = PyPDFLoader(temp_file_path)
        docs = loader.load()

        cleaning = st.checkbox("å¯ç”¨é¢„å¤„ç†ï¼ˆå»é™¤é¡µçœ‰é¡µè„š/å‚è€ƒæ–‡çŒ®ï¼‰", value=True, key="single_doc_clean")
        if cleaning:
            docs = clean_documents(docs)
            st.success("å·²å®Œæˆè®ºæ–‡é¢„å¤„ç†")

        if not st.session_state.summary:
            with st.spinner("æ­£åœ¨ç”Ÿæˆè®ºæ–‡æ‘˜è¦..."):
                summary = summarizer.summarize(docs, strategy='auto')
                st.session_state.summary = summary
        else:
            summary = st.session_state.summary

        st.subheader("ğŸ“ è®ºæ–‡æ‘˜è¦")
        edited_summary = st.text_area("ç¼–è¾‘æ‘˜è¦", value=summary, height=200, key="single_doc_summary")
        if st.button("æ›´æ–°æ‘˜è¦", key="update_summary"):
            st.session_state.summary = edited_summary
            summary = edited_summary
        st.info(summary)

        with st.expander("ğŸ“‘ æŸ¥çœ‹è®ºæ–‡åˆ‡å—è¯¦æƒ…", expanded=False):
            chunks = retriever.list_chunks(docs, preview_chars=300)
            for i, chunk in enumerate(chunks):
                st.markdown(f"### Chunk {i}")
                st.write(f"**å…ƒæ•°æ®**ï¼š")
                st.write(f"- å…¨å±€ç´¢å¼•ï¼š{chunk['metadata'].get('chunk_index', 'æ— ')}")
                st.write(f"- æ ‡é¢˜å±‚çº§ï¼š{chunk['metadata'].get('heading_hierarchy', [])}")
                st.write(f"- æ ‡é¢˜çº§åˆ«ï¼š{chunk['metadata'].get('heading_level', 0)}")
                st.write(f"- æ ‡é¢˜å†…å®¹ï¼š{chunk['heading'] or 'æ— '}")
                st.write(f"**å®Œæ•´å†…å®¹**ï¼š")
                st.text_area(label=f"Chunk {i} å®Œæ•´æ–‡æœ¬", value=chunk['text'], height=300, key=f"chunk_{i}_content")
                st.divider()

        file_hash = hashlib.sha256(file_content).hexdigest()
        index_dir = retriever._index_dir_for_hash(file_hash)
        if os.path.exists(index_dir):
            loaded = retriever.load_index(index_dir)
            if loaded:
                st.info(f"å·²åŠ è½½æœ¬åœ°ç´¢å¼•ï¼š{index_dir}")
            else:
                retriever.build_index(docs, index_dir, overwrite=True)
        else:
            retriever.build_index(docs, index_dir)
            st.success(f"å·²æ„å»ºè®ºæ–‡ç´¢å¼•ï¼š{index_dir}")

        st.subheader("ğŸ”— æ¨èç›¸å…³æ–‡çŒ®")
        col_a, col_b, col_c = st.columns([2,1,1])
        with col_a:
            source = st.selectbox("é€‰æ‹©æ–‡çŒ®æ¥æº", ["CrossRef", "arXiv"], key="rec_source")
        with col_b:
            topk = st.slider("è¿”å›æ•°é‡", min_value=3, max_value=20, value=5, key="rec_topk")
        with col_c:
            rec_button = st.button("ğŸ” æ¨èç›¸å…³æ–‡çŒ®", key="rec_button")

        if rec_button:
            with st.spinner("æ­£åœ¨æŸ¥è¯¢å¤–éƒ¨çŸ¥è¯†åº“..."):
                query_text = extract_search_query_from_docs(docs, summarizer)
                cache_dir = index_dir
                cached = cache_load(cache_dir, source)
                if cached and isinstance(cached, list) and len(cached) >= 1:
                    recs = cached
                else:
                    if source == "CrossRef":
                        recs = fetch_crossref(query_text, topk)
                    else:
                        recs = fetch_arxiv(query_text, topk)
                    cache_save(cache_dir, source, recs)

            if recs:
                st.success(f"ä¸ºä½ çš„è®ºæ–‡æ‰¾åˆ° {len(recs)} æ¡å¯èƒ½ç›¸å…³çš„æ–‡çŒ®ï¼ˆæ¥æºï¼š{source}ï¼‰")
                for i, r in enumerate(recs, start=1):
                    title = r.get('title','')
                    authors = r.get('authors','')
                    venue = r.get('venue','') or r.get('container','') or ''
                    year = r.get('year','')
                    url = r.get('url','') or r.get('link','') or ''
                    doi = r.get('doi','')
                    st.markdown(f"**{i}. {title}**")
                    st.write(f"- **ä½œè€…**ï¼š{authors}")
                    st.write(f"- **æœŸåˆŠ/æ¥æº**ï¼š{venue}  - **å¹´ä»½**ï¼š{year}")
                    if doi:
                        st.write(f"- **DOI**ï¼š{doi}  - **é“¾æ¥**ï¼š{url}")
                    else:
                        st.write(f"- **é“¾æ¥**ï¼š{url}")
                    st.divider()
            else:
                st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ï¼Œè¯·å°è¯•æ›´æ”¹æ¥æºæˆ–å¢åŠ è¿”å›æ•°é‡ã€‚")

        st.subheader("ğŸ’¬ è®ºæ–‡é—®ç­”åŒº")
        for user_msg, assistant_msg in st.session_state.chat_history:
            st.chat_message("user").write(user_msg)
            st.chat_message("assistant").write(assistant_msg)

        query = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼Œæˆ–è¾“å…¥â€œæ¨èç›¸å…³æ–‡çŒ®â€ä»¥è§¦å‘æ–‡çŒ®æ¨èï¼‰", key="single_doc_query")
        if query:
            st.chat_message("user").write(query)
            history_str = ""
            recent_history = st.session_state.chat_history[-MAX_HISTORY_ROUNDS:]
            start_idx = len(st.session_state.chat_history) - len(recent_history) + 1
            for idx, (u, a) in enumerate(recent_history, start_idx):
                history_str += f"è½®æ¬¡{idx}ï¼š\nç”¨æˆ·ï¼š{u}\nåŠ©æ‰‹ï¼š{a}\n\n"
            history_str += f"è½®æ¬¡{len(st.session_state.chat_history) + 1}ï¼š\nç”¨æˆ·ï¼š{query}\n"

            if is_recommend_command(query):
                cmd_source, cmd_topk = parse_recommend_command(query)
                effective_source = cmd_source or source
                effective_topk = cmd_topk or topk
                with st.spinner("æ­£åœ¨æŸ¥è¯¢å¤–éƒ¨çŸ¥è¯†åº“ä»¥æ¨èç›¸å…³æ–‡çŒ®..."):
                    query_text = extract_search_query_from_docs(docs, summarizer)
                    cache_dir = index_dir
                    cached = cache_load(cache_dir, effective_source)
                    if cached and isinstance(cached, list) and len(cached) >= 1:
                        recs = cached[:effective_topk]
                    else:
                        if effective_source == "CrossRef":
                            recs = fetch_crossref(query_text, effective_topk)
                        else:
                            recs = fetch_arxiv(query_text, effective_topk)
                        cache_save(cache_dir, effective_source, recs)
                if recs:
                    st.success(f"ä¸ºä½ çš„è®ºæ–‡æ‰¾åˆ° {len(recs[:effective_topk])} æ¡å¯èƒ½ç›¸å…³çš„æ–‡çŒ®ï¼ˆæ¥æºï¼š{effective_source}ï¼‰")
                    for i, r in enumerate(recs[:effective_topk], start=1):
                        title = r.get('title','')
                        authors = r.get('authors','')
                        venue = r.get('venue','') or r.get('container','') or ''
                        year = r.get('year','')
                        url = r.get('url','') or r.get('link','') or ''
                        doi = r.get('doi','')
                        st.markdown(f"**{i}. {title}**")
                        st.write(f"- **ä½œè€…**ï¼š{authors}")
                        st.write(f"- **æœŸåˆŠ/æ¥æº**ï¼š{venue}  \n- **å¹´ä»½**ï¼š{year}")
                        if doi:
                            st.write(f"- **DOI**ï¼š{doi}  \n- **é“¾æ¥**ï¼š{url}")
                        else:
                            st.write(f"- **é“¾æ¥**ï¼š{url}")
                        st.divider()
                    response = f"å·²æ¨è {len(recs[:effective_topk])} æ¡æ–‡çŒ®ï¼ˆæ¥æºï¼š{effective_source}ï¼‰"
                else:
                    st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ï¼Œè¯·å°è¯•æ›´æ”¹æ¥æºæˆ–å¢åŠ è¿”å›æ•°é‡ã€‚")
                    response = "æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®"
                st.chat_message("assistant").write(response)
                st.session_state.chat_history.append((query, response))
            else:
                with st.spinner("æ­£åœ¨æ£€ç´¢å¹¶ç”Ÿæˆå›ç­”..."):
                    chunks, response = retriever.run(docs, query, history_str)
                st.chat_message("assistant").write("**æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹**ï¼š")
                st.write([c.page_content[:200] + "..." for c in chunks])
                st.chat_message("assistant").write("**å›ç­”**ï¼š")
                st.write(response)
                st.session_state.chat_history.append((query, response))

        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯å†å²", key="clear_chat"):
            st.session_state.chat_history = []
            st.rerun()


def paper_compare_tab(llm, embeddings):
    st.subheader("ğŸ” ä¸¤ç¯‡è®ºæ–‡å¯¹æ¯”åˆ†æ")
    agent = build_paper_compare_agent(llm, embeddings)

    col1, col2 = st.columns(2)
    with col1:
        paper1_file = st.file_uploader("ä¸Šä¼ ç¬¬ä¸€ç¯‡è®ºæ–‡", type="pdf", key="paper1_uploader")
    with col2:
        paper2_file = st.file_uploader("ä¸Šä¼ ç¬¬äºŒç¯‡è®ºæ–‡", type="pdf", key="paper2_uploader")

    if paper1_file and paper2_file:
        paper1_bytes = paper1_file.read()
        paper2_bytes = paper2_file.read()

        initial_state = {
            'paper1_file': paper1_bytes,
            'paper2_file': paper2_bytes,
            'paper1_docs': None,
            'paper2_docs': None,
            'paper1_index': None,
            'paper2_index': None,
            'paper1_core_content': None,
            'paper2_core_content': None,
            'comparison_result': None
        }

        if st.button("ğŸš€ å¼€å§‹å¯¹æ¯”åˆ†æ", key="start_compare"):
            with st.spinner("Agentæ­£åœ¨å¤„ç†ï¼Œè¯·ç¨å€™..."):
                final_state = agent.invoke(initial_state)

        if st.button("ğŸ—‘ï¸ æ¸…ç©ºä¸Šä¼ æ–‡ä»¶", key="clear_papers"):
            st.rerun()


def main():

    st.title("ğŸ“š Yuan2.0 å­¦æœ¯è®ºæ–‡æ™ºèƒ½åŠ©æ‰‹")

    with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹å’ŒåµŒå…¥..."):
        llm, embeddings = get_models()
    st.success("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    tab1, tab2 = st.tabs(["ğŸ“„ å•ç¯‡è®ºæ–‡é—®ç­”", "ğŸ” ä¸¤ç¯‡è®ºæ–‡å¯¹æ¯”"])
    with tab1:
        single_doc_qa_tab(llm, embeddings)
    with tab2:
        paper_compare_tab(llm, embeddings)


if __name__ == '__main__':
    main()
